With the growing need to work with increasingly complex data, modern neural networks have increased the difficulty in interpreting the algorithms used. 
Working with this type of architectures could be very difficult for different reasons: the black-box models used give us the results, but doesn't tell 
the whole story and the results are also difficult to interpret. The implementation uses time-consuming algorithms and the complexity in data increases 
architecture's complexity. No less important is the undefined mathematical framework in which they work: the depth of these architectures involves 
pathologies such as vanishing gradient (the network stops learning) or learning instability.